{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# an RNN model for hangman from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from generic_model import generic_model\n",
    "\n",
    "\n",
    "#generic model contains generic methods for loading and storing a model\n",
    "class RNN(generic_model):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super(RNN, self).__init__(config)\n",
    "\n",
    "        # Store important parameters\n",
    "        self.rnn_name = config['rnn']\n",
    "        self.input_dim = config['vocab_size'] + 1\n",
    "        self.hidden_dim = config['hidden_dim'] \n",
    "        self.num_layers = config['num_layers']\n",
    "        self.embed_dim = config['embedding_dim']\n",
    "        self.output_dim = config['vocab_size']\n",
    "\n",
    "        #whether to use character embeddings\n",
    "        if config['use_embedding']:\n",
    "            self.use_embedding = True\n",
    "            self.embedding = nn.Embedding(self.input_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.use_embedding = False\n",
    "            \n",
    "        #linear layer after RNN output\n",
    "        in_features = config['miss_linear_dim'] + self.hidden_dim*2\n",
    "        mid_features = config['output_mid_features']\n",
    "        self.linear1_out = nn.Linear(in_features, mid_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2_out = nn.Linear(mid_features, self.output_dim)\n",
    "\n",
    "        #linear layer after missed characters\n",
    "        self.miss_linear = nn.Linear(config['vocab_size'], config['miss_linear_dim'])        \n",
    "\n",
    "        #declare RNN\n",
    "        if self.rnn_name == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size=self.embed_dim if self.use_embedding else self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers,\n",
    "                               dropout=config['dropout'],\n",
    "                               bidirectional=True, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=self.embed_dim if self.use_embedding else self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers,\n",
    "                              dropout=config['dropout'],\n",
    "                              bidirectional=True, batch_first=True)\n",
    "\n",
    "        #optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=config['lr'])\n",
    "\n",
    "    def forward(self, x, x_lens, miss_chars):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN\n",
    "        :param x: input tensor of shape (batch size, max sequence length, input_dim)\n",
    "        :param x_lens: actual lengths of each sequence < max sequence length (since padded with zeros)\n",
    "        :param miss_chars: tensor of length batch_size x vocab size. 1 at index i indicates that ith character is NOT present\n",
    "        :return: tensor of shape (batch size, max sequence length, output dim)\n",
    "        \"\"\"        \n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "            \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        # now run through RNN\n",
    "        output, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2, -1, self.hidden_dim)\n",
    "        hidden = hidden[-1]\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "\n",
    "        hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
    "        #project miss_chars onto a higher dimension\n",
    "        miss_chars = self.miss_linear(miss_chars)\n",
    "        #concatenate RNN output and miss chars\n",
    "        concatenated = torch.cat((hidden, miss_chars), dim=1)\n",
    "        #predict\n",
    "        return self.linear2_out(self.relu(self.linear1_out(concatenated)))\n",
    "\n",
    "    def calculate_loss(self, model_out, labels, input_lens, miss_chars, use_cuda):\n",
    "        \"\"\"\n",
    "        :param model_out: tensor of shape (batch size, max sequence length, output dim) from forward pass\n",
    "        :param labels: tensor of shape (batch size, vocab_size). 1 at index i indicates that ith character should be predicted\n",
    "        :param: miss_chars: tensor of length batch_size x vocab size. 1 at index i indicates that ith character is NOT present\n",
    "\t\t\t\t\t\t\tpassed here to check if model's output probability of missed_chars is decreasing\n",
    "        \"\"\"\n",
    "        outputs = nn.functional.log_softmax(model_out, dim=1)\n",
    "        #calculate model output loss for miss characters\n",
    "        miss_penalty = torch.sum(outputs*miss_chars, dim=(0,1))/outputs.shape[0]\n",
    "        \n",
    "        input_lens = input_lens.float()\n",
    "        #weights per example is inversely proportional to length of word\n",
    "        #this is because shorter words are harder to predict due to higher chances of missing a character\n",
    "        weights_orig = (1/input_lens)/torch.sum(1/input_lens).unsqueeze(-1)\n",
    "        weights = torch.zeros((weights_orig.shape[0], 1))    \n",
    "        #resize so that torch can process it correctly\n",
    "        weights[:, 0] = weights_orig\n",
    "\n",
    "        if use_cuda:\n",
    "        \tweights = weights.cuda()\n",
    "        \n",
    "        #actual loss\n",
    "        loss_func = nn.BCEWithLogitsLoss(weight=weights, reduction='sum')\n",
    "        actual_penalty = loss_func(model_out, labels)\n",
    "        return actual_penalty, miss_penalty\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
