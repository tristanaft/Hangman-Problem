{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My hangman file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "import math\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = open(\"words_250000_train.txt\",\"r\")\n",
    "full_dict = tf.read().splitlines()\n",
    "tf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm, I should do a test-train split on the full dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dict_train, dict_test = train_test_split(full_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ughhhh how do I actually code up the AI model? I have the GAME, I need the PLAYER!\n",
    "\n",
    "\n",
    "The model needs to accept:\n",
    "an n-letter word, with letters/blanks encoded as one-hots (n by 27 dense vector)\n",
    "a 1 x 26 vector of guesses made so far (0 for no guess, 1 for guess)\n",
    "\n",
    "it needs to return as output:\n",
    "1 x 26 vector of which argmax is the letter the model \"chooses next\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 170475\n",
      "Max input size: 29\n",
      "Average word length: 9.354416\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "max_input_length = max([len(word) for word in dict_train])\n",
    "batch_size = np.array([len(word) for word in dict_train]).mean()\n",
    "print(\"Training set size: %i\" % len(dict_train))\n",
    "print(\"Max input size: %i\" % max_input_length)\n",
    "print(\"Average word length: %f\" % batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227300"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theopolitics\n",
      "_h_____i_i__\n",
      "['i', 'r', 'm', 'x', 'h', 'w']\n",
      "['e', 'c', 'o', 'l', 't', 's', 'p']\n"
     ]
    }
   ],
   "source": [
    "alphabet = list(string.ascii_lowercase)\n",
    "orig_word = random.choice(dict_train)\n",
    "blanked_word = \"_\" * len(orig_word)\n",
    "#I want a distribution biased towards 0 instead of a flat one for choosing guesses\n",
    "#do I though?\n",
    "num_guesses = round(random.triangular(0,10))\n",
    "guess_list = random.sample(alphabet, num_guesses)\n",
    "\n",
    "if(num_guesses != 0):\n",
    "    for guess_letter in guess_list:\n",
    "        indices = [i for i, x in enumerate(list(orig_word)) if x == guess_letter]\n",
    "        wordList = list(blanked_word)\n",
    "        for i in indices:\n",
    "            wordList[i] = guess_letter #now I replace the occurrences of the letter\n",
    "        blanked_word = \"\".join(wordList)\n",
    "\n",
    "remaining_correct_guesses = list(set(orig_word) - set(guess_list))\n",
    "\n",
    "print(orig_word)\n",
    "print(blanked_word)\n",
    "print(guess_list)\n",
    "print(remaining_correct_guesses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now what, we have input:\n",
    "\n",
    "blanked word -> encode it\n",
    "guess list -> encode it\n",
    "remaining correct guesses *this is the target -> encode it?\n",
    "\n",
    "how do I encode the blanked word though? the strings also have variable length...\n",
    "\n",
    "Ok, convert word into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 26, 26, 26]\n"
     ]
    }
   ],
   "source": [
    "def letter_list_to_number(letter_list):\n",
    "        #this takes a list of letters and returns a list of \n",
    "        out = []\n",
    "        alphabet = list(string.ascii_lowercase)\n",
    "        for letter in letter_list:\n",
    "                if letter == \"_\":\n",
    "                        out.append(27) #putting this here so I can also use it on the hidden word\n",
    "                else:\n",
    "                    out.append(alphabet.index(letter))\n",
    "        return out\n",
    "\n",
    "guess_list_encoded = letter_list_to_number(guess_list)\n",
    "orig_word_encoded = letter_list_to_number(list(orig_word))\n",
    "word = [i if i in guess_list_encoded else 26 for i in orig_word_encoded]\n",
    "obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n",
    "print(word)\n",
    "for i, j in enumerate(word):\n",
    "            #print(i,j)\n",
    "            obscured_word[i, j] = 1\n",
    "#print(obscured_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 27)\n"
     ]
    }
   ],
   "source": [
    "print(obscured_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 25, 6, 16]\n",
      "[13, 8, 12, 1]\n"
     ]
    }
   ],
   "source": [
    "guess_list_encoded = letter_list_to_number(guess_list)\n",
    "orig_word_encoded = letter_list_to_number(list(orig_word))\n",
    "print(guess_list_encoded)\n",
    "print(orig_word_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def letter_list_to_number(letter_list):\n",
    "        out = np.zeros(len(letter_list))\n",
    "        alphabet = list(string.ascii_lowercase)\n",
    "        for letter in letter_list:\n",
    "                if letter == \"_\":\n",
    "                        out.append(27) #putting this here so I can also use it on the hidden word\n",
    "                else:\n",
    "                    np.append(out,alphabet.index(letter))\n",
    "        return out\n",
    "                \n",
    "guess_list_encoded = letter_list_to_number(guess_list)\n",
    "print(guess_list_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_obscured_word(self):\n",
    "        word = [i if i in self.letters_guessed else 26 for i in self.full_word]\n",
    "        obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n",
    "        for i, j in enumerate(word):\n",
    "            obscured_word[i, j] = 1\n",
    "        return(obscured_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'r', 'e', 'l', 'i', 'y', 'a']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(orig_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'r', 't', 'y'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(guess_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'e', 'l', 'i', 'a']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_correct_guesses = list(set(orig_word) - set(guess_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok... I can't feed in the letters one at a time... bummer. I need to pad it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are feeding stuff to torch, stuff has to be in the form n x 1 x 27, since it assumes we are working in batches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(alphabet)+1 #+1 for underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([5, 1, 27])\n"
     ]
    }
   ],
   "source": [
    "# n_letters needs to be 27 to include underscore\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "def letter_to_number(letter):\n",
    "    if letter == \"_\": #this is a blank space\n",
    "        return 27\n",
    "    if letter == \"0\" #this is padding\n",
    "    else:\n",
    "        return alphabet.index(letter)\n",
    "\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters+1> Tensor\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letter_to_number(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <word_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def word_to_tensor(word):\n",
    "    tensor = torch.zeros(len(word), 1, n_letters)\n",
    "    for widx, letter in enumerate(word):\n",
    "        tensor[widx][0][letter_to_number(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letter_to_tensor('a'))\n",
    "\n",
    "print(word_to_tensor('apple').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nimb\n",
      "____\n",
      "['d', 'z', 'g', 'q']\n",
      "['n', 'b', 'm', 'i']\n"
     ]
    }
   ],
   "source": [
    "alphabet = list(string.ascii_lowercase)\n",
    "orig_word = random.choice(dict_train)\n",
    "blanked_word = \"_\" * len(orig_word)\n",
    "#I want a distribution biased towards 0 instead of a flat one for choosing guesses\n",
    "#do I though?\n",
    "num_guesses = round(random.triangular(0,6))\n",
    "guess_list = random.sample(alphabet, num_guesses)\n",
    "\n",
    "if(num_guesses != 0):\n",
    "    for guess_letter in guess_list:\n",
    "        indices = [i for i, x in enumerate(list(orig_word)) if x == guess_letter]\n",
    "        wordList = list(blanked_word)\n",
    "        for i in indices:\n",
    "            wordList[i] = guess_letter #now I replace the occurrences of the letter\n",
    "        blanked_word = \"\".join(wordList)\n",
    "\n",
    "remaining_correct_guesses = list(set(orig_word) - set(guess_list))\n",
    "\n",
    "print(orig_word)\n",
    "print(blanked_word)\n",
    "print(guess_list)\n",
    "print(remaining_correct_guesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_word_length = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 26, 26, 26]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def letter_to_number(letter_list):\n",
    "        #this takes a list of letters and returns a list of \n",
    "        out = []\n",
    "        alphabet = list(string.ascii_lowercase)\n",
    "        for letter in letter_list:\n",
    "                if letter == \"_\":\n",
    "                        out.append(27) #putting this here so I can also use it on the hidden word\n",
    "                else:\n",
    "                    out.append(alphabet.index(letter))\n",
    "        return out\n",
    "\n",
    "guess_list_encoded = letter_list_to_number(guess_list)\n",
    "orig_word_encoded = letter_list_to_number(list(orig_word))\n",
    "word = [i if i in guess_list_encoded else 26 for i in orig_word_encoded]\n",
    "obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n",
    "print(word)\n",
    "for i, j in enumerate(word):\n",
    "            #print(i,j)\n",
    "            obscured_word[i, j] = 1\n",
    "print(obscured_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43malphabet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(alphabet)\n",
      "\u001b[1;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "alphabet.append(\"_\",\"=\")\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_tensor(current_word):\n",
    "    longest_word_length = 45 #this is the longest word in the english language\n",
    "    #first, encode word: ex \"apple\"\n",
    "    #assume word is something like \"_pp__\"\n",
    "    alphabet = list(string.ascii_lowercase) \n",
    "    alphabet.append(\"_\")#this is for blanks\n",
    "    #alphabet.append(\"=\") # using this for generic padding\n",
    "    #input_length = longest_word_length + n_letters+1\n",
    "    num_letters = len(alphabet)\n",
    "\n",
    "    # the current word tensor needs to be encoded as a 1 x 27 x N length tensor\n",
    "    # 1 because we are using batches of 1, 27 because 26 letters + \"_\" and N is the max word length\n",
    "    current_word_tensor = torch.zeros(1,num_letters,longest_word_length)\n",
    "    for widx, letter in enumerate(current_word):\n",
    "        #print(widx)\n",
    "        #print(letter)\n",
    "        current_word_tensor[0][widx][alphabet.index(letter)] = 1\n",
    "    #print(current_word_tensor)\n",
    "    return current_word_tensor\n",
    "\n",
    "def guess_list_to_tensor(guess_list):\n",
    "    #need to encode the guess list as a 1 x 26 tensor\n",
    "    guess_tensor = torch.zeros(1, 1, 26)\n",
    "    for letter in guess_list:\n",
    "        guess_tensor[0][0][alphabet.index(letter)] = 1\n",
    "    \n",
    "    return guess_tensor\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 45])\n",
      "torch.Size([1, 1, 26])\n"
     ]
    }
   ],
   "source": [
    "print(word_to_tensor(\"_pp__\").shape)\n",
    "print(guess_list_to_tensor([\"p\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the current word is converted into a tensor and then fed into a LSTM, the guess list is converted to a tensor and then COMBINED with the output from the LSTM and then that combo is fed into a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Player(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(Player, self).__init__()\n",
    "        self.num_classes = num_classes #number of outputs\n",
    "        self.num_layers = num_layers #I think this is arbitrary\n",
    "        self.input_size =  input_size #input features\n",
    "        self.hidden_size = hidden_size #to match number of letters in encoded guess\n",
    "        self.seq_length = seq_length #to match number of letters in input\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size * 2, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, 26) #fully connected last layer\n",
    "\n",
    "    \n",
    "    def forward(self, encoded_word, encoded_guess):\n",
    "        h_0 = torch.zeros(self.num_layers, encoded_word.size(0), self.hidden_size) #hidden state\n",
    "        c_0 = torch.zeros(self.num_layers, encoded_word.size(0), self.hidden_size) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(encoded_word, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        hn = torch.cat(hn,encoded_guess) #combine data with encoded guess data before passing to dense layer\n",
    "\n",
    "        out = self.fc_1(out) #first Dense\n",
    "\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(alphabet)-1\n",
    "num_layers = 10 #I think this is arbitrary\n",
    "input_size =  len(alphabet)\n",
    "hidden_size = len(alphabet)-1\n",
    "seq_length = longest_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_word_length = 45\n",
    "player1 = Player(num_classes = len(alphabet)-1, num_layers = 10,\n",
    "                input_size =  len(alphabet),\n",
    "                hidden_size = len(alphabet)-1,\n",
    "                seq_length = longest_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Player(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Player, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.guess_tensor = torch.zeros(1, 1, 26)\n",
    "\n",
    "    def forward(self, word_tensor, guess_tensor):\n",
    "        \"\"\"\n",
    "        forward takes word_tensor and \n",
    "        \"\"\"\n",
    "\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = torch.cat(self.h2o(hidden), guess_tensor)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def get_guess_tensor(self, new_gt):\n",
    "        self.guess_tensor = new_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_net(input_obscured_word_seen, input_letters_guessed_previously):\n",
    "    with cntk.layers.default_options(initial_state = 0.1):\n",
    "        lstm_outputs = cntk.layers.Recurrence(cntk.layers.LSTM(MAX_NUM_INPUTS))(input_obscured_word_seen)\n",
    "        final_lstm_output = cntk.ops.sequence.last(lstm_outputs)\n",
    "        combined_input = cntk.ops.splice(final_lstm_output, input_letters_guessed_previously)\n",
    "        dense_layer = cntk.layers.Dense(26, name='final_dense_layer')(combined_input)\n",
    "        return(dense_layer)\n",
    "    \n",
    "input_obscured_word_seen = cntk.ops.input_variable(shape=27,\n",
    "                                                   dynamic_axes=[cntk.Axis.default_batch_axis(),\n",
    "                                                                 cntk.Axis.default_dynamic_axis()],\n",
    "                                                   name='input_obscured_word_seen')\n",
    "input_letters_guessed_previously = cntk.ops.input_variable(shape=26,\n",
    "                                                           dynamic_axes=[cntk.Axis.default_batch_axis()],\n",
    "                                                           name='input_letters_guessed_previously')\n",
    "\n",
    "z = create_LSTM_net(input_obscured_word_seen, input_letters_guessed_previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#input_length = longest_word_length + n_letters+1\n",
    "\n",
    "# Size: [batch_size, seq_len, input_size]\n",
    "input = torch.randn(1, longest_word_length, n_letters)\n",
    "\n",
    "lstm = nn.LSTM(input_size=input_length, hidden_size=512, batch_first=True)\n",
    "\n",
    "output, _ = lstm(input)\n",
    "output.size()  # => torch.Size([12, 384, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(2 * n_letters, n_hidden, n_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the AI attempts to guess a letter that has been chosen? Give a penalty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering here is useless. The words are not grouped together in a way that they are close in terms of guessing in hangman. I have to figure out my own distance metric for this and I don't have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - *hyperlogically:* hyperlogically, hypertrichy, nonhypostatically, supercatholically\n",
      " - *tyroid:* assyroid, boronia, gyroma, hydrophis, pygmoid, rhizoid, syconid, tetraiodid, thoom, throes, thrombosed, throwforward, torpidity, tyburnian, tyroid\n",
      " - *multicrystalline:* multicrystalline, transcrystalline\n",
      " - *unlatched:* tilthead, unclutched, unjapanned, unlatched, unlimned, unmature, unskaithed\n",
      " - *noninterchangeable:* noninterchangeable\n",
      " - *superfluitance:* superfluitance, superincumbency\n",
      " - *youngmannishness:* youngmannishness\n",
      " - *carcinosarcomata:* carcinosarcomata\n",
      " - *greatgrandparent:* greatgrandparent\n",
      " - *pims:* dump, fizz, jmx, limosi, mim, peasy, picco, pijaw, pims, plied, psocids, qms, quiffs, rdbms, simpai, smiris, tpmp, tws, updives, wights\n",
      " - *cystoepiplocele:* cystoepiplocele, glossoepiglottic\n",
      " - *moodys:* anotus, broadways, houndish, mobby, monkism, monogyny, moocha, moodir, moodys, myotomes, topotypes\n",
      " - *cryptoprotestant:* cryptoprotestant\n",
      " - *deeptangled:* deeptangled, deeptroubled, longhandled, smoothankled\n",
      " - *chrysler:* chrysler, chrysotile, coniroster, cresylene, euryclea, oxychlor, threestriper, wheedler\n",
      " - *trichoglossidae:* ptenoglossate, trichoglossidae\n",
      " - *saponin:* aconic, apocynthion, caingin, chapourn, cushioning, fabozzi, pavonian, salpingonasal, saponin, sarodes, scusin, soapstoner, spanlong, spondil, suspension, waggoned\n",
      " - *cruciate:* bicuspidate, churchwards, cornupete, cruciate, crunchily, denunciate, fascinate, frenate, grucche, hydrocyanate, premedicate, structure, verificate\n",
      " - *immigration:* chemisorption, immigration, limonitization, revigoration, semigrainy, subingression\n",
      " - *quasilatin:* brazilein, huascaran, quasiautomatic, quasiexiled, quasilatin\n",
      " - *apologies:* apologetic, apologies, apostatizes, celiotomies, facilities, gabrilowitsch, horologium, prorogues, solpugides, tautologised, willowiest\n",
      " - *cotutor:* acceptors, boggstown, cotoros, cotutor, kevutzoth, lituitoid, perpetuator, pollution, potator, stupor, unoutworn\n",
      " - *mireille:* amarelle, homedale, irresistible, jessieville, martialed, minersville, mireille, renville, sidekicks, vibratiuncle\n",
      " - *anaerobious:* anacanthous, anaerobious, expeditiously, nonferociously, uninjurious\n",
      " - *judicialize:* indisciplined, injudicially, judicialize, tuberclelike\n",
      " - *buskers:* biosphere, buckeroo, burberry, buskers, buskirk, butterine, dousers, duckboard, dunkerque, rushford\n",
      " - *jenica:* bennink, danism, donia, hebraica, irenica, jenica, jennilee, menorca, penide, pennyan, toxicon, vermicular\n",
      " - *prebreathe:* cobreathe, deepbreathing, hotbreathed, prebelieve, prebreathe, prefreshmen\n",
      " - *witlessness:* fiendishness, hastelessness, windlasses, witlessness\n",
      " - *consternation:* conglutination, consternation, consummating, monothalamian, nanoinstruction, nonappropriation\n",
      " - *punctulum:* multititular, punctulum, punctus, puntilla, pushfully, sunsetblue\n",
      " - *tupping:* curbings, drubbing, effusing, hushing, ouphish, outstepping, steppingstone, stobbing, tubkeeping, tupping, upswings, wiping\n",
      " - *progymnospermic:* progymnospermic\n",
      " - *conserved:* accoutered, bantered, casseroles, conserved, exserted, longdesired, mouseweb, nonsilver\n",
      " - *crystallographically:* crystallographically\n",
      " - *incanted:* encallow, encaustic, icecapped, illminded, incanted, inconel, incubuses, inductees, ironred, itinerantly, linkeditted, snowcrowned, uncharge, ungummed, warantee\n",
      " - *coilability:* addability, coilability, colloidality, communicability, existibility, namability, reversability, squeezability\n",
      " - *uncement:* conoscenti, exacerbescent, innermost, truncheons, unaccept, unbeseem, uncement, uncombative, unfoaming, unsonant\n",
      " - *nonservile:* booneville, inconceptible, nonnegligent, nonostensible, nonservile, nonsynodic, otherwhile, powderlike, slatersville\n",
      " - *ultrabrachycephalic:* ultrabrachycephalic\n",
      " - *oballa:* billa, ebarta, galbula, hexacoralla, mobula, notasulga, oballa, oxalylurea, tobacco, umbellula\n",
      " - *songman:* honeyman, longarm, moreland, nonflagrant, roadman, sightsman, slipman, snuffman, sonata, songman, strymon\n",
      " - *idolo:* alditol, alikuluf, dolose, drylot, dulcor, eidology, fairyology, hollow, idolo, idolomancy, nicol, niduli, psychro, simblot\n",
      " - *nonconfidence:* circumfluence, improficiency, nonconfidence\n",
      " - *stela:* datedly, entelechy, fema, hyosternal, lovely, ostend, otlf, pamella, postfetal, prelacy, ptelea, shikari, sieve, slatelike, sleazo, softly, sstv, steepen, stela, virelai, zweig\n",
      " - *unremonstrant:* contraremonstrance, uncorroborant, unremonstrant\n",
      " - *nephoscope:* anaphylactogen, nephoscope, tachoscope\n",
      " - *upsnatch:* ninnywatch, restitch, splatch, testmatch, uppowoc, upsnatch, upstraight, yokkaichi\n",
      " - *subsistent:* presubsistent, seaserpent, southwesterner, subediting, subfastigiated, subsistent, superpatient\n",
      " - *designless:* degradingness, designedness, designless, distributress, expenseless, feelingness, geoscientists, inflectionless, legibleness, physicianless, wreathless\n",
      " - *beldamship:* beldamship, burgessship, househeadship, vintnership\n",
      " - *retabulate:* absquatulate, decarbonate, recapitulatory, retabulate, revalidate\n",
      " - *crimes:* acrinyl, braises, bridesmaid, brigid, caddiced, carburizes, cesiums, chinbeak, commos, corbeils, crimes, cruses, cyprians, ficaries, rages, rhymes, ritzes, seimas\n",
      " - *nonpredatory:* bronchodilator, nondeprecatory, nonpredatory\n",
      " - *penfeathered:* gembedizened, greathearted, hoaryfeathered, penfeathered\n",
      " - *losingly:* atingle, flyingly, hookswinging, insincerely, lochinvar, losingly, postaxially, pryingly\n",
      " - *denominable:* denominable, denominationalize, disinheritable, harmonisable, reclaimable\n",
      " - *unadorableness:* avowableness, nonsolubleness, quadrateness, unadorableness, uncompliableness, undecayedness, unseasonableness\n",
      " - *denten:* advected, canteen, cowden, danker, debug, deem, delanson, denizate, denten, depths, destains, dillymen, diphen, odense, pentheus, plenipotency, pontin, redeem, saxten, venta\n",
      " - *heterotype:* antetypes, hemiorthotype, heterodactylous, heterodoxness, heterogeny, heterotype\n",
      " - *hesperornithoid:* hesperornithoid\n",
      " - *ethnomusicologist:* endocrinologist, ethnomusicologist\n",
      " - *yellowsprinkled:* yellowsprinkled\n",
      " - *preadministrative:* maladministrative, preacquisitive, preadministrative\n",
      " - *bursty:* buibui, burnley, bursty, durums, fleuretty, gustav, inbursts, untasty\n",
      " - *slitting:* bloating, brittling, clueing, dislimning, oliveburg, schtick, shiftingly, signifying, slitting, spitful, whitefringed\n",
      " - *bone:* aaee, bag, barocco, bigboned, bone, bonum, boranes, bowwow, bronzine, bscom, bubonocele, buffo, bunnie, cong, gorge, huge, hymned, koln, line, losey, nordine, pene, raband, roethke, tome, unbonnet\n",
      " - *tribophosphorescent:* tribophosphorescent\n",
      " - *standage:* atahualpa, attendance, stagecraft, standage, standaway, stanhopes, stormable, stravage\n",
      " - *ladyishly:* haggishly, ladyishly, lampistry, languished\n",
      " - *cistrans:* cistrans, cystosyrinx, dieticians, disintegrant, dissuasions, fisticuffs, membrana, mistrace, obstipant\n",
      " - *aleman:* abepp, adlumia, aggadah, akamai, alar, aleman, allsopp, almaine, alopias, apatan, evaleen, farmann, flamenco, flookan, halfwomanly, levyne, paleomagnetism, polemician, trademark\n",
      " - *roa:* bfd, brea, dph, drona, erika, erp, goad, ingroup, isa, jae, juxon, lomax, mosra, neurogram, poc, provola, rebab, roa, robbi, sql\n",
      " - *heater:* archaeolater, breadmaker, dishearten, eleganter, hatter, headpin, heater, hebete, helmuth, meager, muktear, pheasantseye, pitted, reister, relaters, resawer, rheinlander, vertebral, whewt, whilter, zitter\n",
      " - *choanophorous:* cacophonous, choanophorous, cistophorus, proanaphoral, rhizophagous\n",
      " - *lethals:* anencephaly, catcalls, ethynes, flenchgut, inthrals, jawfall, kinetoplast, kitkahaxki, leachable, leewill, lethals, letta, neckfast, newsgirls, nikethamide, overhangs, sleuthful\n",
      " - *cardinalis:* cardinalis, cardiologist, gallinuline, parepididymis, subcardinal, tachylalia\n",
      " - *angiotonic:* allassotonic, amphictyonic, angiotonic, anticouncil, auditoria, digitogenin, dynamogenic, endotropic, holotonic, polyganglionic\n",
      " - *hacking:* badging, braying, crackling, darkening, derricking, duckbill, fulcruming, gabling, hacking, haglike, halcyons, hedychium, hirpling, hobbing, hockingport, jauking, knifings, landini, linkping, lyricking, outvaluing, packinghouse, zincifying\n",
      " - *digraphic:* agraphia, circassic, diarrhoetic, digraphic, myographic, pasigraphic, pathographical, vapographic, vitrophyre\n",
      " - *presetting:* breakthrough, freeacting, pirouetting, preseptal, presetting, pressurefixing, profityielding, protonemertini, pyriphlegethon, tradeseeking, wormresembling\n",
      " - *counterpart:* countercomplaints, counterpart, counterwrite\n",
      " - *sealery:* decury, defiler, diseaseful, dwayberry, dwellers, espieglerie, isospory, salieri, salmary, sciametry, sealery, sensately, sesser, shallowy, sheppey, snakedrawn, solebury, styler, wellesz\n",
      " - *mythopoeist:* bathmotropism, encyclopaedist, hemathidrosis, mythopoeist, mythopoetizing, oryctologist\n",
      " - *dehumanize:* anapaganize, deamericanize, decemberish, dehumanising, dehumanize, thumbnail, unharmonized\n",
      " - *vicepresidentship:* vicepresidentship\n",
      " - *keeves:* beckemeyer, beevish, cleuks, keavy, keeves, metered, serenades, shopkeepers, sleeves, theelols, zibeths\n",
      " - *shapiest:* croupiest, plumiest, poetpriest, queaziest, shafiite, shagginess, shapiest, shipwright, shopfolk, snapperback, snippiness, traplight, wittiest\n",
      " - *megapodiidae:* megapodiidae, meliphagidan, nectariniidae\n",
      " - *overfondness:* newfangledness, ovariocyesis, overcorruptly, overfondness, overtartness\n",
      " - *nonforeknowledge:* nonforeknowledge\n",
      " - *indan:* amphipodan, clorinda, cujam, dinkas, eident, enoplan, handy, indan, indio, infusorian, iridal, journ, masan, misdemeanour, onlap, pandan, rhineodon, snogs, tiptap, unpaint, virgin\n",
      " - *chromatinic:* borosalicylic, chiromantical, chromatinic, chronomantic, chronotropic, ichthulinic, perisplanchnic, psychrophilic\n",
      " - *ransome:* brisure, broadsouled, canzones, chancesown, consomme, drainpipe, hailstone, hansel, leptome, ramshorns, ransackle, ransome, sunrooms, wansith, winsomely\n",
      " - *vetitive:* advective, bepimple, demifigure, executively, kivikivi, punitive, twentyfive, veinwise, vergence, vetitive\n",
      " - *postcondition:* mispronunciation, osteochondritis, photoinduction, postclavicular, postcondition\n",
      " - *arbor:* abrogator, agrimotor, ambonnay, arbor, argiope, arly, arvol, bairnwort, bibcock, cartboot, enarbour, fairport, shabbos, sybow, yearbird\n",
      " - *economite:* chondrodite, decongestive, dedolomitized, economite, ecotone, eschynite, metarossite, photomappe, scleromata, stakhanovite, uncanonize\n",
      " - *hypokalemic:* hepatotoxemia, hyperaesthetic, hypochnaceae, hypokalemic\n",
      " - *antiphon:* antiphon, antipodagron, pantihose, snowshoing, tryptophan\n",
      " - *muns:* alants, clunky, duds, ekts, guars, gulfs, hubb, isnt, kulak, luise, mangos, mdqs, meng, muns, munsey, pung, quey, scant, smich\n",
      " - *celisse:* celisse, ceviche, clewiston, dolisie, ilissus, lowlinesses, matisse, menuisier, placelessly, premisses, wellposted\n",
      " - *outrace:* autofrettage, contracter, cuprate, extruct, hydrase, leatrice, outbacker, outbarter, outdance, outdraft, outofstater, outrace, outreasons, outrocked, palaeostraca, petrarch, portrayer, summable\n",
      " - *unresenting:* animadverting, fuerteventura, nonirritating, redesignating, ultrabelieving, unabsorbing, unapprehending, unprojecting, unresenting, unweighing\n",
      " - *toluylenediamine:* toluylenediamine\n",
      " - *starched:* researches, scraighed, searaven, shaggycoated, spearheads, stacket, starched, starkbecalmed, starleaved, sulphureted\n",
      " - *pachycephalous:* dolichocephalize, pachycarpous, pachycephalous\n",
      " - *cophosis:* anacoenoses, bolsheviks, calypsos, cephalodymus, cophosis, cornhusks, cytophagic, glossolysis, lophiomyidae, medicophysics, seroprognosis, vasoneurosis\n",
      " - *ethnopsychic:* ethnohistoric, ethnopsychic, syphilopsychosis\n",
      " - *formless:* formless, fortressing, garbless, lobbyists, lowclass, normalness, reformist, tireless\n",
      " - *overreflectiveness:* overreflectiveness\n",
      " - *undisparity:* nondisparities, unactuality, undisparity\n",
      " - *ichthyopaleontology:* ichthyopaleontology\n",
      " - *unterraced:* aforegranted, interdicts, interplacental, lamebrained, ovateserrated, unadulteratedly, underbalanced, unintersected, unterraced, viperheaded\n",
      " - *subterritories:* scoterythrous, subequalities, subterraneanize, subterritories\n",
      " - *thoughtfree:* phosphuret, roughpaved, straightfibered, thoughtfree, thoughtstirring\n",
      " - *inseparably:* indefeasibly, inobediently, inseparably, insufferable, unsupernaturally\n",
      " - *piniform:* bigbloom, ciliiform, einkorn, lopatnikoff, manifolds, nonuniformly, pannuscorium, pileworm, piniform, vinifera\n",
      " - *overidden:* coresidual, hydrindene, overawful, overdeck, overgirdle, overidden, shovelbladed\n",
      " - *washcolored:* raisincolored, tilecovered, washcolored\n",
      " - *outspokenly:* correspondently, foulspoken, outspokenly\n",
      " - *foiled:* affixes, boilerful, coopted, fantailed, feuillee, flushed, foibles, foiled, foisted, fomites, formedon, fugger, horned, podley, reaffixed\n",
      " - *matroofed:* albatrosses, dampproofer, gastronomies, macropod, matronize, matroofed\n",
      " - *pseudogothic:* pseudogothic, pseudomoslem, purpuroxanthin\n",
      " - *slenderer:* menderes, polejumper, selfenamored, seventyfooter, slangrell, sleepdrunk, slenderer, slenderfooted, splenocolic, statuebordered, sunflower\n",
      " - *recoding:* belching, dehorting, germforming, openhousing, reaccompany, reacidify, rebrandish, rechosen, recoding, rectocele, repaying, reticulin, revilingly, rhodiums, rockclimb, rustremoving, seascouting, secondine\n",
      " - *precontemn:* frenchgrown, grootfontein, mascouten, praelectress, precontemn, precontent, prestonsburg, triconodonta\n",
      " - *anisoleucocytosis:* anisoleucocytosis\n",
      " - *arret:* adject, afferent, argental, arret, asbest, barolet, darmit, earhart, fathercraft, favrot, lareena, lyres, manlet, perroquet, rewarrant\n",
      " - *zoologicobotanical:* zoologicobotanical\n",
      " - *guttiness:* auctioneer, droughtiness, emptiness, filthinesses, grittiness, guttiferal, guttiness, intestineness, moistiness, multiprocess, outlinear, snubbiness, staginess, survivers, unbountifulness\n",
      " - *nikolainkaupunki:* nikolainkaupunki\n",
      " - *calography:* calography, clanjamphrey, radiograph, sciography, sculpturally\n",
      " - *grillage:* drainage, gillale, graybarked, griffade, grillage, trolleyer\n",
      " - *extremistic:* dysphemistic, epigrammatic, expressmen, extratelluric, extremistic, pointillistic\n",
      " - *unmisunderstanding:* unmisunderstanding\n",
      " - *theatricalizing:* overbrutalizing, strengthinspiring, theatricalizing\n",
      " - *unexpectable:* instructable, unassociable, unexemptable, unexpectable, unexplicable\n",
      " - *unleal:* enclear, quaternal, unidead, unleal, unlegible, unsceptical, unseason, unshady, utley\n",
      " - *nonhierarchical:* nonarithmetical, nonhierarchical, trierarchal\n",
      " - *harnessed:* aridnesses, burnettized, everblessed, graynesses, hairylegged, hardnosed, hareeyed, harnessed, harquebusade, marcheshvan, shabbinesses, taxlessly, tyrannised\n",
      " - *pegmatophyre:* neurapophysial, pegmatophyre, phaeophyta, spermatozoio\n",
      " - *swinked:* ginned, plinker, plunked, sconced, sewickley, shirker, silked, spinages, swanky, swiggers, swinked\n",
      " - *anthdia:* absinthial, anabia, anodynia, anthdia, antidorcas, arthuriana, gnathotheca, gonopodia, rhinthonica, vivaria\n",
      " - *syllidae:* artamidae, nymphalidae, scolytidae, smallvisaged, subline, syllabary, syllabized, syllidae, synecdoche, unsyllabled\n",
      " - *appearing:* acipenserine, aggrandising, anschauung, appalachians, appearing, appeasements, aspergillin, attiring, bespreading, cooperating, cupmaking, opalescing, oppugning, peacepreaching, upperco, wallpapering, yeaning, zephyrinus\n",
      " - *coccidium:* caciquism, coccidium, coccoidal, conditorium, monopodium, onchidium, semicupium\n",
      " - *diplospondyli:* asterospondyli, diplospondyli\n",
      " - *poplarism:* colorum, hoplomachist, krypticism, mogilalism, poplarism, postpalatine, postpyloric, poulardize, protoplasmic, whitleyism\n",
      " - *melinis:* jerkings, jubilist, martinic, medially, medicining, melinis, melitemia, melonlike, milinda, myelocytosis, petulancies, prelithic\n",
      " - *transmeridionally:* transmeridionally\n",
      " - *foray:* boards, coccyx, colddraw, dorri, enwrap, foray, forehatchway, fouqu, gorraf, horn, nynorsk, pomfrey, scrapy, wordy, zootaxy, zoril\n",
      " - *ayn:* abc, achen, acned, asio, avow, ayahs, ayn, azo, clwyd, decyl, dyad, fev, gayyou, lactyl, qv, tn, vagi, wauf, wyson, ye\n",
      " - *oralize:* adalie, aribine, civilizes, crystalize, drawlink, glottalize, oralize, raftlike\n",
      " - *worldaught:* worldaught, worldvalid, worsetaught\n",
      " - *thorniness:* churchliness, librarianess, thenness, thermonasty, thorniness, thoroughbass, tricksiness, vapourishness\n",
      " - *unvariedly:* unluxuriantly, unreprovedly, unvariedly, weariedly\n",
      " - *lichenizing:* alcoholising, clothesdrying, lichenizing, misbeginning\n",
      " - *extubation:* exorbitation, exsufflation, extubation, objurgation, obnubilation, pituitaries\n",
      " - *underdive:* adessive, inclusive, selfdefensive, unalleviative, underbellies, underdive, underhold, underjudged, undernourish, unillusive, unsteadiness, wanderlust\n",
      " - *chamma:* ammi, cacka, cahuapana, chamlet, chamma, chinghai, cloacean, euchlaena, guazuma, mohammad, phantasmal, phasma\n",
      " - *fortier:* boggier, festivalgoer, firstchop, flier, forethrift, forewing, formaliter, fortier, foujdar, furriery, hirsuties, northener\n",
      " - *repeals:* firebreaks, rangeheads, reanalysis, reboots, redressal, refers, refreeze, repayal, repeals, repegged, replans, ruperta\n",
      " - *templates:* completely, epoptes, exstemporaneous, reenslaves, teleplotter, templates, tempuras, triptanes\n",
      " - *disecondary:* discongruity, disecondary, prelegendary\n",
      " - *electrocorticogram:* electrocorticogram, electromotivity\n",
      " - *connectional:* connectional, nondictionary, overemotional, publicational\n",
      " - *trash:* athbash, crany, creagh, derah, igdrasil, kurdish, lash, resh, stomach, tasbih, transferor, trash, trickish, whisk\n",
      " - *gracewood:* casewood, framework, gracewood, gradefinder, gravelous, trachypteroid, wraparound\n",
      " - *semiepically:* estrogenically, isotopically, semiepically, semifinals, semimetaphorical, semipiscine, servomechanically\n",
      " - *undecreasing:* mispurchasing, undecreasing, undepressive, underlapping, undisclosing, unimpressibly, unsuccumbing\n",
      " - *essentialism:* commercialised, essentialism, interventionism, monumentalism, polysynthesism\n",
      " - *serohemorrhagic:* serohemorrhagic\n",
      " - *woodser:* booklear, bookseller, cooksburg, toolsetter, windsor, woodser, wunder\n",
      " - *porulous:* acajous, columbous, embryulcus, exsuccous, porulous, spinulous, sporiparous, sportulae, torulous\n",
      " - *limivorous:* limboinfantum, limivorous, malodorous, semihumorous, tubiflorous\n",
      " - *wieren:* beerup, ewerer, gienah, impregn, lippen, pereon, quiescence, silkscreen, theremin, truerun, twicepretended, upperer, weippe, wheel, wieren, wilmont, winzes, wire\n",
      " - *nonestimably:* monostichous, nonestimably, noninstrumentally, nonrescissible, unclassifiably, uninhabitably\n",
      " - *presentation:* commentitious, overregimentation, perscrutation, prefunctional, presentation, reforestization, repersonalization, representativeship, tressilation\n",
      " - *schizostele:* schizognathae, schizostele, siphonostele\n",
      " - *optometric:* altometer, cyclometric, entomotaxy, graptolitic, nonphonemic, optometric, osphresiometry\n",
      " - *manurable:* cranioaural, equable, manageable, manurable, menopause, monodrame, unburnable, undryable, unscrambler\n",
      " - *ballard:* baccalaurean, backarrow, ballard, ballastic, bicapsular, bullweed, bylaws, cowyard, galleys, galloon, lallapalooza, szilard, yellowbark\n",
      " - *tarnation:* alineation, autocremation, disincarnation, grammarian, latration, pantagruelion, planation, pygmalion, tarnation, thaxton, threadfin, throatswollen, toopatient, valuationally\n",
      " - *parage:* carvage, floorage, gasbags, image, lovages, pang, parage, paraguayans, pardaos, paregmenon, parried, phragmocone, plumbagos, portages, prunase, pyorrheic, shargel, spartacan, wastage\n",
      " - *cartaceous:* calumnious, candyfloss, cartaceous, catalysts, cryptocerous, furfuraceous, sabiaceous, trifarious\n",
      " - *uncoincided:* snakeengirdled, uncoincided, uncoincidently, unconsumed\n",
      " - *reparative:* declarative, denarcotize, hyperactive, readaptive, reappraised, redrive, reparative, somniative, supportive, treasuretrove\n",
      " - *archchristendom:* archchristendom\n",
      " - *fedders:* endears, fedders, feederin, fibbers, fiddleneck, redeemeress, referrers\n",
      " - *lowboughed:* clodbreaker, lowboughed, shallowthoughted\n",
      " - *melliferous:* celluliferous, helminthous, hexaspermous, machinehour, melliferous, nonsiliceous, soldierbush\n",
      " - *traced:* blackeyed, chuted, creamfaced, drawerful, flustrated, franckot, spaces, tachy, traced, tragedies, tragicomedies, trauchled, trowel, twicet\n",
      " - *repertorial:* epicortical, hyperborean, pertusaria, prepositorial, repertorial, spermicidal\n",
      " - *eleutherophobia:* eleutherophobia\n",
      " - *peristerite:* epistolarily, hyalosiderite, lorenzenite, painterlike, periesophageal, peristerite, periuranium, physeteroidea\n",
      " - *laccolith:* blackcoat, calculist, hippolyta, laccolith, plateaulith, sackcloth\n",
      " - *phototachometrical:* photochemigraphy, phototachometrical\n",
      " - *banatite:* anaptychi, apathize, banatite, banlieue, benzamide, bonewhite, byzantian, cantalite, choanite, dramatical, goniatites, guanajuatite, pragmatists, vauxite, yajnopavita\n",
      " - *beballed:* baseballs, beballed, beefaloes, beetlehead, behooved, benzylic, ebenales, fatbellied, metallic, singlecelled, unescalloped, vervelled\n",
      " - *kerneling:* curtailing, equaling, interfiling, keelblock, kephallina, kerneling, skraeling, towerdwelling\n",
      " - *intertransformability:* intertransformability\n",
      " - *sobersault:* nonoccult, sobersault, summersault\n",
      " - *misdivide:* crassitude, dipeptide, individed, misdivide, misedited, muscovade, unmisgiving\n",
      " - *taira:* anogra, cairn, catarrhal, clima, contrafissura, kamakura, laired, larva, manbria, nacarat, qadiriya, taira, thibaut, tioga, tobira, turk, valva, zanily\n",
      " - *maleate:* amalekite, balete, caveae, galitea, maccaboy, madebeaver, maleate, malvie, megalocyte, melonladen, mulejenny, ramenta, rowletts, selfdebate, simonette, smallmouth, stakemaster, valvule, wayleave\n",
      " - *bertie:* beatrice, bellies, beloit, beltwise, beneme, berake, bermudian, berths, bertie, bessi, birdwise, heartgrief, rerefief\n",
      " - *phyllobranchiate:* phyllobranchiate, psychopanychite\n",
      " - *volcanology:* ichthyology, membranology, volcanology, volumenometry\n",
      " - *selfpositing:* selfapplauding, selfconvicting, selfdetermining, selfplaying, selfpositing, selfrenouncing, semiconducting, sulfatizing\n",
      " - *gassier:* basifier, gassier, grisbet, jagheer, mainswear, masscult, mossie, russifier\n",
      " - *overpopulousness:* contemptuousness, overpopulous, overpopulousness, vexatiousness\n",
      " - *malarioid:* algarrobin, aluminide, amalgamationist, foolhardihood, galeorchis, halfsolid, illqualified, malaprop, malarioid, marmaritin, plupatriotic, saviorhood, tamarisks, taperpointed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import distance\n",
    "    \n",
    "#words = \"YOUR WORDS HERE\".split(\" \") #Replace this line\n",
    "#words = dict_train\n",
    "\n",
    "#lets try a small subset of dict_train first to make sure this works.....\n",
    "words = random.sample(dict_train, round(len(dict_train)/100))\n",
    "\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "lev_similarity = np.asarray(lev_similarity)\n",
    "\n",
    "affprop = AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "\n",
    "affprop.fit(lev_similarity.astype(float))\n",
    "#affprop.fit(words)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([182,  11,   2, ...,  52,  36, 117], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import distance\n",
    "    \n",
    "#words = \"YOUR WORDS HERE\".split(\" \") #Replace this line\n",
    "#words = dict_train\n",
    "\n",
    "#lets try a small subset of dict_train first to make sure this works.....\n",
    "words = random.sample(dict_train, round(len(dict_train)/1000))\n",
    "\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, -13, -12, ..., -13, -12, -12],\n",
       "       [-13,   0, -11, ..., -10, -10, -13],\n",
       "       [-12, -11,   0, ...,  -8,  -8,  -9],\n",
       "       ...,\n",
       "       [-13, -10,  -8, ...,   0, -10, -13],\n",
       "       [-12, -10,  -8, ..., -10,   0,  -9],\n",
       "       [-12, -13,  -9, ..., -13,  -9,   0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_extra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn_extra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMedoids\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_extra'"
     ]
    }
   ],
   "source": [
    "from sklearn_extra.cluster import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_train)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I think the best way to do this would be to make a decision tree from scratch, but I think there is an issue of outliers that I am not super sure how to deal with. I think the dictionary is big enough that there are some words that the algorithm should figure out how to throw out. Like if there are 2 moves left, it should make the highest probability assumption it can which eliminates trees with more than 2 moves of surprise.\n",
    "\n",
    "So... I think how the algorithm should work is, it doesnt just pick the ONE letter that would maximize the entropy next, it should find like a PATH of guesses that maximally reduces the entropy.\n",
    "\n",
    "I am assuming there are always 6 tries, which seems a bit low..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a bit complex though... If I build a tree I don't have to constantly re-calculate it, but I just move from node to node.\n",
    "\n",
    "HOW DO I MAKE A DECISION TREE CLASSIFIER WITH NO LABELS THOUGH?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, I want to just remake hangman here so I can mess with it. So gut all of the connection to some external anything. I think I know how it works from running it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHangman2(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.guessed_letters = []\n",
    "        #should include a self word and self correct word\n",
    "        self.word = \"\"\n",
    "        self.correct_word = \"\"\n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []\n",
    "        self.has_won = False\n",
    "        \n",
    "    def generate_correct_word(self):\n",
    "        self.correct_word = random.choice(self.full_dictionary)\n",
    "\n",
    "    def get_winstate(self):\n",
    "        return self.has_won\n",
    "\n",
    "\n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
    "        all_letters = [x[0] for x in self.full_dictionary_common_letter_sorted]\n",
    "\n",
    "\n",
    "        ###############################################\n",
    "        # Replace with your own \"guess\" function here #\n",
    "        ###############################################\n",
    "\n",
    "        # clean the word so that we strip away the space characters\n",
    "        # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
    "        clean_word = word.replace(\" \", \"\") #I am putting this here in case I forget later to remove the space stuff when I paste this into the solution\n",
    "        clean_word = word.replace(\"_\",\".\")\n",
    "        #print(word)\n",
    "        #print(word[::2])\n",
    "        #print(clean_word)\n",
    "\n",
    "        # find length of passed word\n",
    "        len_word = len(clean_word)\n",
    "\n",
    "        # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
    "        current_dictionary = self.full_dictionary\n",
    "\n",
    "        new_dictionary = []\n",
    "\n",
    "        # iterate through all of the words in the old plausible dictionary\n",
    "        for dict_word in current_dictionary:\n",
    "            # continue if the word is not of the appropriate length\n",
    "            if len(dict_word) != len_word:\n",
    "                continue\n",
    "                \n",
    "            #NOTE THIS MAY NOT BE HOW I DO THIS AFTER I DO THE STUFF BELOW!\n",
    "\n",
    "            # if dictionary word is a possible match then add it to the current dictionary\n",
    "            if re.match(clean_word,dict_word):\n",
    "                new_dictionary.append(dict_word)\n",
    "\n",
    "        # overwrite old possible words dictionary with updated version\n",
    "        current_dictionary = new_dictionary\n",
    "\n",
    "\n",
    "        # count occurrence of all characters in possible word matches\n",
    "        full_dict_string = \"\".join(new_dictionary)                 \n",
    "\n",
    "        guess_letter = '!'\n",
    "\n",
    "\n",
    "        # what options do we still have to guess?\n",
    "        letter_options = [letter for letter in all_letters if letter not in self.guessed_letters]\n",
    "\n",
    "        # break down the possibilities by letter if that letter is correct\n",
    "        # future_guess_array = []\n",
    "        # guess_entropy_array = []\n",
    "\n",
    "        def calc_entropy(base, subset): \n",
    "            # This calculates the entropy of taking a set A and breaking it down into a subset B and B complement\n",
    "            # I mean mathematically, this acts on lists rather sets programming wise.\n",
    "\n",
    "            #I have domain issues if p=1 or p=0, so if the base == subset or if subset empty\n",
    "            if base == subset: #hmmmmmm how do I handle this?\n",
    "                return 1\n",
    "            elif len(subset) == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                p = len(subset) / len(base)\n",
    "                return -(p*math.log(p) + (1-p)*math.log(1-p))\n",
    "                \n",
    "        # for each letter, calculate the entropy of choosing that letter\n",
    "        word_count_total = len(current_dictionary)\n",
    "        guess_entropy_array = []\n",
    "\n",
    "        for letter in letter_options:\n",
    "            #dict_by_letter = []\n",
    "            word_count_by_letter = 0\n",
    "\n",
    "            # This process here is very slow, for each letter it counts how many words in the dictionary have that letter present.\n",
    "            # I think there is a better way to do this...\n",
    "            for word in current_dictionary:\n",
    "                if set(letter) & set(word): # I use sets to get all the words with a specific letter\n",
    "                    #dict_by_letter.append(word)\n",
    "                    word_count_by_letter+=1\n",
    "            \n",
    "            \n",
    "            p = word_count_by_letter / word_count_total\n",
    "\n",
    "\n",
    "            # p=0 and p=1 are special and need to treat them differently, they are out of the domain of log\n",
    "            # p=0 means there are no words remaining in the dictionary, so that guess is useless and I set the entropy to 0\n",
    "            # p=1 will only occur when the number of words corresponding to a letter is the same as in the entire dictionary\n",
    "            # this will only occur when we have locked in on one possible answer, so I set the entropy to 1 which is greater than the normal max the entropy can have\n",
    "\n",
    "            if p == 0:\n",
    "                letter_entropy = 0\n",
    "            elif p == 1:\n",
    "                letter_entropy = 1\n",
    "            else:\n",
    "                letter_entropy = -(p*math.log(p) + (1-p)*math.log(1-p))\n",
    "            #future_guess_array.append(dict_by_letter)\n",
    "            #guess_entropy = calc_entropy(current_dictionary, dict_by_letter)\n",
    "            #guess_entropy_array.append(guess_entropy)\n",
    "            guess_entropy_array.append(letter_entropy)\n",
    "\n",
    "        # now, which letter had the highest entropy?\n",
    "        idx_max = guess_entropy_array.index(max(guess_entropy_array))\n",
    "        best_letter = letter_options[idx_max]\n",
    "        guess_letter = best_letter\n",
    "                    \n",
    "        return guess_letter\n",
    "\n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "\n",
    "    #what do I need to replace the responses?\n",
    "    #get a word\n",
    "    #get remaining tries\n",
    "    #print number of tries remaining\n",
    "    #go through the process of resolving a guess\n",
    "    #\n",
    "\n",
    "    def handle_guess(self, guessLetter, currentWord, ans):\n",
    "        #I have to convert the strings to lists, find indices of the letter (if present) and then smush the lists back into strings and spit it out\n",
    "        indices = [i for i, x in enumerate(list(ans)) if x == guessLetter]\n",
    "        wordList = list(currentWord)\n",
    "        for i in indices:\n",
    "            wordList[i] = guessLetter #now I replace the occurrences of the letter\n",
    "        word = \"\".join(wordList)\n",
    "        return word\n",
    "        \n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        self.has_won = False\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "\n",
    "        #pick a word from the dictionary at random.\n",
    "        #correctWord = random.choice(self.full_dictionary) #this is the initial word, but need to update it based on guesses\n",
    "        self.generate_correct_word()\n",
    "        word = \"_\" * len(self.correct_word) #I should make this a member of self right?\n",
    "\n",
    "        tries_remains = 6 #I think this needs to just be a magic number?\n",
    "        if verbose:\n",
    "            print(\"Successfully start a new game! # of tries remaining: {0}. Word: {1}.\".format(tries_remains, word))\n",
    "        while tries_remains >0:\n",
    "            #tries_remains -=1 tries remains decreases after WRONG answer\n",
    "\n",
    "            #get guessed letter\n",
    "            guess_letter = self.guess(word)\n",
    "\n",
    "            #append guessed letter to guessed letters field in hangman object\n",
    "            self.guessed_letters.append(guess_letter)\n",
    "            if verbose:\n",
    "                print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "\n",
    "            #apply guessed letter to the word\n",
    "            new_word = self.handle_guess(guess_letter, word, self.correct_word) \n",
    "\n",
    "            #check if we have the word\n",
    "            if new_word == self.correct_word:\n",
    "                word = new_word\n",
    "                if verbose:\n",
    "                    print(\"Success! the word was: %s\" % word)\n",
    "                self.has_won = True\n",
    "                break\n",
    "            \n",
    "            if new_word == word:\n",
    "                #this means that we did not get a correct guess\n",
    "                #decrease number of tries\n",
    "                if verbose:\n",
    "                    print(\"Incorrect guess {0}, # of tries remaining: {1}. Word: {2}.\".format(guess_letter,tries_remains, word))\n",
    "                tries_remains -=1\n",
    "            else:\n",
    "                #We have a correct letter guessed, but not the complete word\n",
    "                #don't decrement tries_remains\n",
    "                word = new_word\n",
    "                if verbose:\n",
    "                    print(\"Got a Letter, {0}, # of tries remaining: {1}. Word: {2}.\".format(guess_letter,tries_remains, word))\n",
    "        if tries_remains == 0:\n",
    "            if verbose:\n",
    "                print(\"You Lose, the answer was: %s\" % self.correct_word)\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\"\"           \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "    \"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully start a new game! # of tries remaining: 6. Word: ________.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 6. Word: r_______.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 6. Word: r_a_____.\n",
      "Guessing letter: s\n",
      "Incorrect guess s, # of tries remaining: 6. Word: r_a_____.\n",
      "Guessing letter: d\n",
      "Got a Letter, d, # of tries remaining: 5. Word: r_ad____.\n",
      "Guessing letter: o\n",
      "Got a Letter, o, # of tries remaining: 5. Word: road____.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 5. Word: road___e.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 5. Word: road_i_e.\n",
      "Guessing letter: l\n",
      "Got a Letter, l, # of tries remaining: 5. Word: roadli_e.\n",
      "Guessing letter: k\n",
      "Success! the word was: roadlike\n"
     ]
    }
   ],
   "source": [
    "game = MyHangman2()\n",
    "game.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_winstate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Game 100, elapsed time is 85.118947, current winrate is 90.909091\n",
      "On Game 200, elapsed time is 167.148824, current winrate is 89.447236\n",
      "On Game 300, elapsed time is 250.472179, current winrate is 88.294314\n",
      "On Game 400, elapsed time is 327.258479, current winrate is 89.223058\n",
      "On Game 500, elapsed time is 412.823485, current winrate is 88.577154\n",
      "On Game 600, elapsed time is 494.952882, current winrate is 86.811352\n",
      "On Game 700, elapsed time is 572.957901, current winrate is 86.552217\n",
      "On Game 800, elapsed time is 645.716178, current winrate is 86.357947\n",
      "On Game 900, elapsed time is 730.106674, current winrate is 86.651835\n",
      "On Game 1000, elapsed time is 806.408744, current winrate is 86.186186\n",
      "Final Score: 86.200000 percent wins. Time elapsed: 806.408744\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "wincount = 0\n",
    "game = MyHangman2()\n",
    "t = time.time()\n",
    "\n",
    "for i in range(N):\n",
    "    elapsed = time.time() - t\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(\"On Game %i, elapsed time is %f, current winrate is %f\" % (i+1, elapsed ,100*wincount/(i)))\n",
    "    game.start_game(verbose = False)\n",
    "    wincount += game.get_winstate()\n",
    "    #if(game.get_winstate()):\n",
    "    #    print(\"Won game %i, win count = %i, time elapsed = %f\" % (i+1, wincount, elapsed))\n",
    "print(\"Final Score: %f percent wins. Time elapsed: %f\" % (100 * wincount / N, elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with word_count method, takes 8.2 secs for 10 games... 18 sec for 25 games\n",
    "\n",
    "With the large lists, takes 9.0 secs for 10 games... 27 sec for 25 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully start a new game! # of tries remaining: 6. Word: ___________.\n",
      "Guessing letter: n\n",
      "Got a Letter, n, # of tries remaining: 6. Word: _n_________.\n",
      "Guessing letter: l\n",
      "Got a Letter, l, # of tries remaining: 6. Word: _n_______l_.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 6. Word: _n____r__l_.\n",
      "Guessing letter: b\n",
      "Got a Letter, b, # of tries remaining: 6. Word: _n____r_bl_.\n",
      "Guessing letter: i\n",
      "Incorrect guess i, # of tries remaining: 6. Word: _n____r_bl_.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 5. Word: _n____rabl_.\n",
      "Guessing letter: y\n",
      "Incorrect guess y, # of tries remaining: 5. Word: _n____rabl_.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 4. Word: _ns___rabl_.\n",
      "Guessing letter: u\n",
      "Got a Letter, u, # of tries remaining: 4. Word: uns__urabl_.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 4. Word: unse_urable.\n",
      "Guessing letter: c\n",
      "Success! the word was: unsecurable\n"
     ]
    }
   ],
   "source": [
    "game = MyHangman3()\n",
    "game.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully start a new game! # of tries remaining: 6. Word: _________.\n",
      "Guessing letter: n\n",
      "Incorrect guess n, # of tries remaining: 6. Word: _________.\n",
      "Guessing letter: s\n",
      "Incorrect guess s, # of tries remaining: 5. Word: _________.\n",
      "Guessing letter: o\n",
      "Got a Letter, o, # of tries remaining: 4. Word: _o_______.\n",
      "Guessing letter: t\n",
      "Got a Letter, t, # of tries remaining: 4. Word: to_______.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 4. Word: torr_____.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 4. Word: torr__i__.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 4. Word: torre_ie_.\n",
      "Guessing letter: d\n",
      "Got a Letter, d, # of tries remaining: 4. Word: torre_ied.\n",
      "Guessing letter: f\n",
      "Success! the word was: torrefied\n",
      "Successfully start a new game! # of tries remaining: 6. Word: _________.\n",
      "Guessing letter: n\n",
      "Incorrect guess n, # of tries remaining: 6. Word: _________.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 5. Word: ______s_s.\n",
      "Guessing letter: o\n",
      "Incorrect guess o, # of tries remaining: 5. Word: ______s_s.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 4. Word: __a_a_s_s.\n",
      "Guessing letter: t\n",
      "Incorrect guess t, # of tries remaining: 4. Word: __a_a_s_s.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 3. Word: _ia_a_s_s.\n",
      "Guessing letter: p\n",
      "Got a Letter, p, # of tries remaining: 3. Word: _iapa_s_s.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 3. Word: _iapa_ses.\n",
      "Guessing letter: u\n",
      "Got a Letter, u, # of tries remaining: 3. Word: _iapauses.\n",
      "Guessing letter: d\n",
      "Success! the word was: diapauses\n",
      "Successfully start a new game! # of tries remaining: 6. Word: ___________.\n",
      "Guessing letter: n\n",
      "Got a Letter, n, # of tries remaining: 6. Word: ______n____.\n",
      "Guessing letter: l\n",
      "Incorrect guess l, # of tries remaining: 6. Word: ______n____.\n",
      "Guessing letter: r\n",
      "Incorrect guess r, # of tries remaining: 5. Word: ______n____.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 4. Word: ______ns___.\n",
      "Guessing letter: o\n",
      "Incorrect guess o, # of tries remaining: 4. Word: ______ns___.\n",
      "Guessing letter: m\n",
      "Incorrect guess m, # of tries remaining: 3. Word: ______ns___.\n",
      "Guessing letter: h\n",
      "Got a Letter, h, # of tries remaining: 2. Word: ______nsh__.\n",
      "Guessing letter: p\n",
      "Got a Letter, p, # of tries remaining: 2. Word: ______nsh_p.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 2. Word: _i_i__nship.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 2. Word: _i_i_enship.\n",
      "Guessing letter: t\n",
      "Got a Letter, t, # of tries remaining: 2. Word: _iti_enship.\n",
      "Guessing letter: c\n",
      "Got a Letter, c, # of tries remaining: 2. Word: citi_enship.\n",
      "Guessing letter: z\n",
      "Success! the word was: citizenship\n",
      "Successfully start a new game! # of tries remaining: 6. Word: _______.\n",
      "Guessing letter: n\n",
      "Incorrect guess n, # of tries remaining: 6. Word: _______.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 5. Word: _a_____.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 5. Word: _a__i__.\n",
      "Guessing letter: e\n",
      "Incorrect guess e, # of tries remaining: 5. Word: _a__i__.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 4. Word: _a__is_.\n",
      "Guessing letter: r\n",
      "Incorrect guess r, # of tries remaining: 4. Word: _a__is_.\n",
      "Guessing letter: h\n",
      "Got a Letter, h, # of tries remaining: 3. Word: _a__ish.\n",
      "Guessing letter: m\n",
      "Incorrect guess m, # of tries remaining: 3. Word: _a__ish.\n",
      "Guessing letter: t\n",
      "Got a Letter, t, # of tries remaining: 2. Word: _at_ish.\n",
      "Guessing letter: f\n",
      "Got a Letter, f, # of tries remaining: 2. Word: _atfish.\n",
      "Guessing letter: b\n",
      "Incorrect guess b, # of tries remaining: 2. Word: _atfish.\n",
      "Guessing letter: c\n",
      "Success! the word was: catfish\n",
      "Successfully start a new game! # of tries remaining: 6. Word: ____________.\n",
      "Guessing letter: n\n",
      "Incorrect guess n, # of tries remaining: 6. Word: ____________.\n",
      "Guessing letter: l\n",
      "Incorrect guess l, # of tries remaining: 5. Word: ____________.\n",
      "Guessing letter: c\n",
      "Got a Letter, c, # of tries remaining: 4. Word: _____c______.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 4. Word: ___ssc______.\n",
      "Guessing letter: u\n",
      "Incorrect guess u, # of tries remaining: 4. Word: ___ssc______.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 3. Word: _r_ssc___r__.\n",
      "Guessing letter: d\n",
      "Got a Letter, d, # of tries remaining: 3. Word: _r_ssc___r_d.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 3. Word: _r_ssc__ered.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 3. Word: _rassc__ered.\n",
      "Guessing letter: o\n",
      "Got a Letter, o, # of tries remaining: 3. Word: _rassco_ered.\n",
      "Guessing letter: g\n",
      "Got a Letter, g, # of tries remaining: 3. Word: grassco_ered.\n",
      "Guessing letter: v\n",
      "Success! the word was: grasscovered\n",
      "Successfully start a new game! # of tries remaining: 6. Word: __________.\n",
      "Guessing letter: n\n",
      "Got a Letter, n, # of tries remaining: 6. Word: ______n___.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 6. Word: ___s__n_s_.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 6. Word: ___s_an_s_.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 6. Word: _r_s_an_s_.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 6. Word: _r_s_anis_.\n",
      "Guessing letter: p\n",
      "Got a Letter, p, # of tries remaining: 6. Word: pr_spanis_.\n",
      "Guessing letter: h\n",
      "Got a Letter, h, # of tries remaining: 6. Word: pr_spanish.\n",
      "Guessing letter: e\n",
      "Success! the word was: prespanish\n",
      "Successfully start a new game! # of tries remaining: 6. Word: ___________.\n",
      "Guessing letter: n\n",
      "Incorrect guess n, # of tries remaining: 6. Word: ___________.\n",
      "Guessing letter: l\n",
      "Got a Letter, l, # of tries remaining: 5. Word: ______l____.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 5. Word: s_____l_s__.\n",
      "Guessing letter: r\n",
      "Incorrect guess r, # of tries remaining: 5. Word: s_____l_s__.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 4. Word: s___i_lis__.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 4. Word: s_e_i_lise_.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 4. Word: s_e_ialise_.\n",
      "Guessing letter: c\n",
      "Got a Letter, c, # of tries remaining: 4. Word: s_ecialise_.\n",
      "Guessing letter: d\n",
      "Got a Letter, d, # of tries remaining: 4. Word: s_ecialised.\n",
      "Guessing letter: p\n",
      "Success! the word was: specialised\n",
      "Successfully start a new game! # of tries remaining: 6. Word: ____________.\n",
      "Guessing letter: n\n",
      "Got a Letter, n, # of tries remaining: 6. Word: ___________n.\n",
      "Guessing letter: l\n",
      "Incorrect guess l, # of tries remaining: 6. Word: ___________n.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 5. Word: s__s_______n.\n",
      "Guessing letter: c\n",
      "Got a Letter, c, # of tries remaining: 5. Word: s__s_c_____n.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 5. Word: s__sic_____n.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 5. Word: s_esic___e_n.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 5. Word: s_esic___ean.\n",
      "Guessing letter: o\n",
      "Got a Letter, o, # of tries remaining: 5. Word: s_esic_o_ean.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 5. Word: s_esic_orean.\n",
      "Guessing letter: t\n",
      "Got a Letter, t, # of tries remaining: 5. Word: stesic_orean.\n",
      "Guessing letter: h\n",
      "Success! the word was: stesichorean\n",
      "Successfully start a new game! # of tries remaining: 6. Word: __________.\n",
      "Guessing letter: n\n",
      "Got a Letter, n, # of tries remaining: 6. Word: _________n.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 6. Word: _______s_n.\n",
      "Guessing letter: r\n",
      "Incorrect guess r, # of tries remaining: 6. Word: _______s_n.\n",
      "Guessing letter: i\n",
      "Incorrect guess i, # of tries remaining: 5. Word: _______s_n.\n",
      "Guessing letter: a\n",
      "Incorrect guess a, # of tries remaining: 4. Word: _______s_n.\n",
      "Guessing letter: e\n",
      "Got a Letter, e, # of tries remaining: 3. Word: _e_____sen.\n",
      "Guessing letter: m\n",
      "Incorrect guess m, # of tries remaining: 3. Word: _e_____sen.\n",
      "Guessing letter: h\n",
      "Got a Letter, h, # of tries remaining: 2. Word: _e___h_sen.\n",
      "Guessing letter: c\n",
      "Got a Letter, c, # of tries remaining: 2. Word: _e__ch_sen.\n",
      "Guessing letter: o\n",
      "Got a Letter, o, # of tries remaining: 2. Word: _e__chosen.\n",
      "Guessing letter: l\n",
      "Got a Letter, l, # of tries remaining: 2. Word: _ellchosen.\n",
      "Guessing letter: f\n",
      "Incorrect guess f, # of tries remaining: 2. Word: _ellchosen.\n",
      "Guessing letter: w\n",
      "Success! the word was: wellchosen\n",
      "Successfully start a new game! # of tries remaining: 6. Word: ________.\n",
      "Guessing letter: n\n",
      "Incorrect guess n, # of tries remaining: 6. Word: ________.\n",
      "Guessing letter: r\n",
      "Got a Letter, r, # of tries remaining: 5. Word: __r_____.\n",
      "Guessing letter: s\n",
      "Got a Letter, s, # of tries remaining: 5. Word: __r_s___.\n",
      "Guessing letter: i\n",
      "Got a Letter, i, # of tries remaining: 5. Word: __r_s_i_.\n",
      "Guessing letter: a\n",
      "Got a Letter, a, # of tries remaining: 5. Word: _ar_s_i_.\n",
      "Guessing letter: p\n",
      "Got a Letter, p, # of tries remaining: 5. Word: _ar_s_ip.\n",
      "Guessing letter: d\n",
      "Got a Letter, d, # of tries remaining: 5. Word: _ards_ip.\n",
      "Guessing letter: h\n",
      "Got a Letter, h, # of tries remaining: 5. Word: _ardship.\n",
      "Guessing letter: b\n",
      "Incorrect guess b, # of tries remaining: 5. Word: _ardship.\n",
      "Guessing letter: w\n",
      "Success! the word was: wardship\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    game.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfull_dict\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_dict' is not defined"
     ]
    }
   ],
   "source": [
    "full_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
